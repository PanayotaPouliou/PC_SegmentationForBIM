{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO9PRr1tTkrs"
      },
      "source": [
        "#Building Point Cloud Segmentation for Architectural Elements *(Walls, Roofs, and Floors)*\n",
        "\n",
        "This Colab notebook provides a comprehensive pipeline for segmenting 3D point cloud data of buildings to identify key architectural elements: walls, roofs, and floors. Utilizing deep learning, this model allows for accurate segmentation, making it easier to analyze and understand building structures from point cloud data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfT0lZP1Tkrw"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "5IWT6BUFWVlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks6NZI63Tkrw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import umap\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.layers import Layer\n",
        "from keras import layers\n",
        "\n",
        "from keras.regularizers import Regularizer\n",
        "from keras.models import load_model\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sys import platlibdir\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrWY3laBbgMA"
      },
      "source": [
        "##Connecting to Gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G25yFHBobfOS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_j2yrS_Tkry"
      },
      "source": [
        "# Loading the dataset\n",
        "\n",
        "We parse the dataset metadata in order to easily map model categories to their\n",
        "respective directories and segmentation classes to colors for the purpose of\n",
        "visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt0pqO9qTkry"
      },
      "outputs": [],
      "source": [
        "with open(\"insert path of your dataset here\") as json_file:\n",
        "    metadata = json.load(json_file)\n",
        "\n",
        "print(metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S--u2Ix5Tkry"
      },
      "outputs": [],
      "source": [
        "points_dir = \"path of the data/BAPC/PartAnnotation/{}/points\".format(\n",
        "    metadata[\"Housing\"][\"directory\"]\n",
        ")\n",
        "labels_dir = \"path of the data/BAPC/PartAnnotation/{}/points_label\".format(\n",
        "    metadata[\"Housing\"][\"directory\"]\n",
        ")\n",
        "LABELS = metadata[\"Housing\"][\"lables\"]\n",
        "COLORS = metadata[\"Housing\"][\"colors\"]\n",
        "\n",
        "VAL_SPLIT = 0.2\n",
        "NUM_SAMPLE_POINTS = 1024\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 60\n",
        "INITIAL_LR = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RD6GMjb6HNia"
      },
      "outputs": [],
      "source": [
        "point_clouds= []\n",
        "point_cloud_labels, all_labels = [], []\n",
        "all_cat, label_data_cat = [], []\n",
        "\n",
        "points_files = glob(os.path.join(points_dir, \"*.pts\"))\n",
        "\n",
        "#i=0\n",
        "\n",
        "for point_file in tqdm(points_files):\n",
        "    seg_file= point_file.replace(\"points/\", \"points_label/\").replace(\".pts\", \".seg\")\n",
        "    point_cloud = np.loadtxt(point_file)\n",
        "    point_cloud_labels = np.loadtxt(seg_file).astype(\"float32\")\n",
        "\n",
        "    point_clouds.append(point_cloud)\n",
        "    all_labels.append(point_cloud_labels)\n",
        "\n",
        "    label_data_cat = keras.utils.to_categorical(point_cloud_labels, num_classes=len(LABELS) + 1)\n",
        "\n",
        "    all_cat.append(label_data_cat)\n",
        "\n",
        "    #i=i+1\n",
        "\n",
        "    #if i==50:\n",
        "    #  break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXvfVPaqYU9_"
      },
      "outputs": [],
      "source": [
        "print(f\"point_clouds[0].shape:\", point_clouds[0].shape)\n",
        "print(f\"all_labels[0].shape:\", all_labels[0].shape)\n",
        "print(f\"all_cat[0].shape:\", all_cat[0].shape)\n",
        "\n",
        "print(f\"nr of point_clouds\", len(point_clouds),f\"nr of labels\",  len(all_labels), f\"nr of onehot encoding labels\", len(all_cat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q7qX24FTkr0"
      },
      "outputs": [],
      "source": [
        "for index in tqdm(range(len(point_clouds))):\n",
        "    current_point_cloud = point_clouds[index]\n",
        "    current_label_cloud = all_cat[index]\n",
        "    current_labels = all_labels[index]\n",
        "    num_points = len(current_point_cloud)\n",
        "    # Randomly sampling respective indices.\n",
        "    sampled_indices = random.sample(list(range(num_points)), NUM_SAMPLE_POINTS)\n",
        "    # Sampling points corresponding to sampled indices.\n",
        "    sampled_point_cloud = np.array([current_point_cloud[i] for i in sampled_indices])\n",
        "    # Sampling corresponding one-hot encoded labels.\n",
        "    sampled_label_cloud = np.array([current_label_cloud[i] for i in sampled_indices])\n",
        "    # Sampling corresponding labels for visualization.\n",
        "    sampled_labels = np.array([current_labels[i] for i in sampled_indices])\n",
        "    # Normalizing sampled point cloud.\n",
        "    norm_point_cloud = sampled_point_cloud - np.mean(sampled_point_cloud, axis=0)\n",
        "    norm_point_cloud /= np.max(np.linalg.norm(norm_point_cloud, axis=1))\n",
        "    point_clouds[index] = norm_point_cloud\n",
        "    all_cat[index] = sampled_label_cloud\n",
        "    all_labels[index] = sampled_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"point_clouds[0].shape:\", point_clouds[0].shape)\n",
        "print(f\"all_labels[0].shape:\", all_labels[0].shape)\n",
        "print(f\"all_cat[0].shape:\", all_cat[0].shape)\n",
        "\n",
        "print(f\"nr of point_clouds\", len(point_clouds),f\"nr of labels\",  len(all_labels), f\"nr of onehot encoding labels\", len(all_cat))"
      ],
      "metadata": {
        "id": "VFtaXDXdJP7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clustering the point clouds"
      ],
      "metadata": {
        "id": "xx_NYowHUj_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Extraction with UMAP\n",
        "*Kmeans and DBSCAN clustering*"
      ],
      "metadata": {
        "id": "8VVxuCj7aVoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UMAP (Uniform Manifold Approximation and Projection) was chosen for dimensionality reduction because it effectively captures complex, non-linear relationships in our dataset while preserving both local and global structures. Unlike traditional linear methods like PCA, UMAP maintains the local neighborhood relationships crucial for point cloud data, ensuring that points close in high-dimensional space remain so in the reduced space. This capability leads to clearer visualizations with distinct cluster separations, making it easier to interpret how different classes of point clouds are distributed and related to one another. Ultimately, UMAP provides a more nuanced understanding of the data, which is essential for our analysis and insights."
      ],
      "metadata": {
        "id": "S99DA5D_qfaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_point_clouds(point_clouds):\n",
        "    return np.array([point_cloud.flatten() for point_cloud in point_clouds])\n",
        "\n",
        "def extract_umap_features(point_clouds, n_components=2):\n",
        "    flattened_point_clouds = flatten_point_clouds(point_clouds)\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(flattened_point_clouds)\n",
        "\n",
        "    reducer = umap.UMAP(n_components=n_components, random_state=42)\n",
        "    umap_features = reducer.fit_transform(scaled_features)\n",
        "\n",
        "    return umap_features\n",
        "\n",
        "def apply_kmeans_clustering(umap_features, n_clusters=10):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels = kmeans.fit_predict(umap_features)\n",
        "\n",
        "    return labels\n",
        "\n",
        "def apply_dbscan_clustering(umap_features, eps=0.5, min_samples=5):\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = dbscan.fit_predict(umap_features)\n",
        "\n",
        "    return labels\n",
        "\n",
        "def visualize_clustering_results(umap_features, kmeans_labels, dbscan_labels):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # K-Means Visualization\n",
        "    axs[0].scatter(umap_features[:, 0], umap_features[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.7)\n",
        "    axs[0].set_title(\"K-Means Clustering\")\n",
        "    axs[0].set_xlabel(\"UMAP Component 1\")\n",
        "    axs[0].set_ylabel(\"UMAP Component 2\")\n",
        "\n",
        "    # DBSCAN Visualization\n",
        "    unique_labels = np.unique(dbscan_labels)\n",
        "    for label in unique_labels:\n",
        "        if label == -1:\n",
        "            color = 'black'  # Noise points\n",
        "        else:\n",
        "            color = plt.cm.viridis(label / len(unique_labels))  # Normalize for colormap\n",
        "\n",
        "        axs[1].scatter(umap_features[dbscan_labels == label, 0],\n",
        "                       umap_features[dbscan_labels == label, 1],\n",
        "                       color=color, label=f'Cluster {label}' if label != -1 else 'Noise', alpha=0.7)\n",
        "\n",
        "    axs[1].set_title(\"DBSCAN Clustering\")\n",
        "    axs[1].set_xlabel(\"UMAP Component 1\")\n",
        "    axs[1].set_ylabel(\"UMAP Component 2\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Extract UMAP features from the point clouds\n",
        "umap_features = extract_umap_features(point_clouds, n_components=2)\n",
        "\n",
        "# Apply K-Means on the extracted UMAP features\n",
        "kmeans_labels = apply_kmeans_clustering(umap_features, n_clusters=10)\n",
        "\n",
        "# Apply DBSCAN on the extracted UMAP features\n",
        "dbscan_labels = apply_dbscan_clustering(umap_features, eps=0.5, min_samples=5)\n",
        "\n",
        "# Visualize the clustering results\n",
        "visualize_clustering_results(umap_features, kmeans_labels, dbscan_labels)\n"
      ],
      "metadata": {
        "id": "KWUgaAjJaT9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans outperformed DBSCAN in our dataset primarily due to the nature of the data distribution and the characteristics of the clustering algorithms. KMeans is particularly effective when dealing with well-separated, spherical clusters, which is often the case in point cloud data where distinct groups are formed by different classes. It relies on centroids and minimizes the variance within each cluster, allowing it to efficiently group similar points together. On the other hand, DBSCAN, which identifies clusters based on density, struggles in datasets with varying densities or in the presence of noise and outliers. Our dataset likely contained clusters that were clearly defined, making KMeans a better fit as it effectively partitioned the data into meaningful groupings, leading to more coherent and interpretable clusters compared to the more flexible but less deterministic approach of DBSCAN."
      ],
      "metadata": {
        "id": "EFLLCjTKqzLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vizualization of different clusters samples"
      ],
      "metadata": {
        "id": "AzDKbEjbq7_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_point_clouds_by_cluster(point_clouds, labels, title=\"Point Clouds by Cluster\"):\n",
        "    unique_labels = np.unique(labels)\n",
        "    unique_labels = unique_labels[unique_labels != -1]  # Exclude noise for visualization\n",
        "\n",
        "    num_clusters = len(unique_labels)\n",
        "    num_cols = min(num_clusters, 3)  # Set max columns to 3\n",
        "    num_rows = (num_clusters + num_cols - 1) // num_cols  # Calculate rows needed\n",
        "\n",
        "    # Use a colormap to assign colors to clusters\n",
        "    cmap = plt.get_cmap(\"tab10\")  # Choose a colormap with distinct colors (up to 10 clusters)\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 4 * num_rows))\n",
        "\n",
        "    for i, label in enumerate(unique_labels):\n",
        "        # Find the indices of point clouds corresponding to this label\n",
        "        cluster_indices = np.where(labels == label)[0]\n",
        "\n",
        "        if len(cluster_indices) > 0:\n",
        "            # Select the first point cloud for this cluster\n",
        "            cluster_point_cloud = point_clouds[cluster_indices[0]]  # Use the first index\n",
        "\n",
        "            # Set a unique color for each cluster\n",
        "            color = cmap(i % 10)  # Cycle through colors if more than 10 clusters\n",
        "\n",
        "            # Create a subplot for each cluster\n",
        "            ax = fig.add_subplot(num_rows, num_cols, i + 1, projection='3d')\n",
        "            ax.scatter(cluster_point_cloud[:, 0], cluster_point_cloud[:, 1], cluster_point_cloud[:, 2],\n",
        "                       color=color, alpha=0.8, edgecolors='w', s=30)  # Adjust colors and style\n",
        "            ax.set_title(f\"Cluster {label}\")\n",
        "            ax.set_xlabel(\"X\")\n",
        "            ax.set_ylabel(\"Y\")\n",
        "            ax.set_zlabel(\"Z\")\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize one point cloud from each K-Means cluster\n",
        "visualize_point_clouds_by_cluster(point_clouds, kmeans_labels, title=\"K-Means Clustering: One Point Cloud per Cluster\")\n"
      ],
      "metadata": {
        "id": "HpW9iKkY26m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Statistical Analysis of UMAP features"
      ],
      "metadata": {
        "id": "LdFDDIzorDfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_statistical_analysis_umap(umap_features, labels):\n",
        "    # Convert UMAP features to a DataFrame for easier analysis\n",
        "    df = pd.DataFrame(umap_features, columns=[f'UMAP Component {i+1}' for i in range(umap_features.shape[1])])\n",
        "    df['Cluster'] = labels\n",
        "\n",
        "    # Calculate statistics for each cluster\n",
        "    summary_stats = df.groupby('Cluster').agg(['mean', 'std', 'min', 'max', 'count'])\n",
        "\n",
        "    return summary_stats\n",
        "\n",
        "# Perform analysis\n",
        "summary_stats_umap = cluster_statistical_analysis_umap(umap_features, kmeans_labels)\n",
        "\n",
        "# Display the statistical summary for each cluster\n",
        "print(\"Statistical Summary for Each Cluster (K-Means):\")\n",
        "print(summary_stats_umap)\n"
      ],
      "metadata": {
        "id": "PnSBzQbp48XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_umap_distributions_by_cluster(umap_features, labels):\n",
        "    # Create a DataFrame for easy plotting\n",
        "    df = pd.DataFrame(umap_features, columns=['UMAP Component 1', 'UMAP Component 2'])\n",
        "    df['Cluster'] = labels\n",
        "\n",
        "    # Melt the DataFrame for seaborn\n",
        "    df_melted = pd.melt(df, id_vars='Cluster', var_name='UMAP Component', value_name='Value')\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(data=df_melted, x='UMAP Component', y='Value', hue='Cluster', palette=\"Set2\")\n",
        "    plt.title(\"UMAP Component Distributions by Cluster (Box Plot)\")\n",
        "    plt.ylabel(\"Component Values\")\n",
        "    plt.xlabel(\"UMAP Components\")\n",
        "    plt.legend(title='Cluster')\n",
        "    plt.show()\n",
        "\n",
        "# Run the function to visualize\n",
        "plot_umap_distributions_by_cluster(umap_features, kmeans_labels)\n"
      ],
      "metadata": {
        "id": "KqTX0HuI52TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mean_umap_by_cluster(umap_features, labels):\n",
        "    # Create a DataFrame for easy plotting\n",
        "    df = pd.DataFrame(umap_features, columns=['UMAP Component 1', 'UMAP Component 2'])\n",
        "    df['Cluster'] = labels\n",
        "\n",
        "    # Calculate mean for each UMAP component by cluster\n",
        "    means = df.groupby('Cluster').mean()\n",
        "\n",
        "    # Plotting\n",
        "    means.plot(kind='bar', figsize=(10, 6), colormap=\"viridis\")\n",
        "    plt.title(\"Mean UMAP Component Values by Cluster\")\n",
        "    plt.ylabel(\"Mean Value\")\n",
        "    plt.xlabel(\"UMAP Components\")\n",
        "    plt.legend(title='Cluster')\n",
        "    plt.show()\n",
        "\n",
        "# Run the function to visualize\n",
        "plot_mean_umap_by_cluster(umap_features, kmeans_labels)\n"
      ],
      "metadata": {
        "id": "aB1tVyZ08cA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_heatmap_umap_stats_by_cluster(umap_features, labels, stat='mean'):\n",
        "    # Create a DataFrame for easy plotting\n",
        "    df = pd.DataFrame(umap_features, columns=['UMAP Component 1', 'UMAP Component 2'])\n",
        "    df['Cluster'] = labels\n",
        "\n",
        "    # Calculate desired statistics for each cluster\n",
        "    if stat == 'mean':\n",
        "        stats = df.groupby('Cluster').mean()\n",
        "    elif stat == 'std':\n",
        "        stats = df.groupby('Cluster').std()\n",
        "    else:\n",
        "        raise ValueError(\"Stat must be 'mean' or 'std'\")\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(stats, annot=True, cmap='coolwarm', cbar_kws={'label': f'{stat.capitalize()} Value'})\n",
        "    plt.title(f\"Heatmap of {stat.capitalize()} UMAP Component Values by Cluster\")\n",
        "    plt.xlabel(\"UMAP Components\")\n",
        "    plt.ylabel(\"Cluster\")\n",
        "    plt.show()\n",
        "\n",
        "# Run the function to visualize mean and standard deviation\n",
        "plot_heatmap_umap_stats_by_cluster(umap_features, kmeans_labels, stat='mean')\n",
        "plot_heatmap_umap_stats_by_cluster(umap_features, kmeans_labels, stat='std')\n"
      ],
      "metadata": {
        "id": "nRi6V4tx8kbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Balancing the distribution of different point clouds in the dataset\n",
        "###KNN sampling"
      ],
      "metadata": {
        "id": "Lrqwwd1H-VKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_undersample_3d(point_clouds, labels, label_list, categorical_label_list):\n",
        "    # Ensure inputs are NumPy arrays\n",
        "    point_clouds = np.array(point_clouds)\n",
        "    labels = np.array(labels)\n",
        "    label_list = np.array(label_list)\n",
        "    categorical_label_list = np.array(categorical_label_list)\n",
        "\n",
        "    # Save the shape of each point cloud (e.g., [n_points, 3])\n",
        "    original_shape = point_clouds.shape[1:]\n",
        "    flattened_point_clouds = np.array([pc.flatten() for pc in point_clouds])\n",
        "\n",
        "    # Identify unique labels and determine the minimum count for undersampling\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    min_count = counts.min()\n",
        "\n",
        "    balanced_point_clouds = []\n",
        "    balanced_labels = []\n",
        "    balanced_label_list = []\n",
        "    balanced_categorical_label_list = []\n",
        "\n",
        "    for label in unique_labels:\n",
        "        # Get samples and their associated label lists for the current label\n",
        "        cluster_samples = flattened_point_clouds[labels == label]\n",
        "        cluster_label_list = label_list[labels == label]\n",
        "        cluster_categorical_label_list = categorical_label_list[labels == label]\n",
        "\n",
        "        # Compute centroid of the cluster\n",
        "        cluster_centroid = cluster_samples.mean(axis=0)\n",
        "\n",
        "        # Use KNN to find closest points to the centroid\n",
        "        knn = NearestNeighbors(n_neighbors=min_count)\n",
        "        knn.fit(cluster_samples)\n",
        "        closest_indices = knn.kneighbors([cluster_centroid], return_distance=False)[0]\n",
        "\n",
        "        # Collect the closest samples and associated label lists\n",
        "        selected_samples = cluster_samples[closest_indices]\n",
        "        selected_label_list = cluster_label_list[closest_indices]\n",
        "        selected_categorical_label_list = cluster_categorical_label_list[closest_indices]\n",
        "\n",
        "        # Reshape back to 3D and extend the balanced lists\n",
        "        balanced_point_clouds.extend(selected_samples.reshape(-1, *original_shape))\n",
        "        balanced_labels.extend([label] * min_count)\n",
        "        balanced_label_list.extend(selected_label_list)\n",
        "        balanced_categorical_label_list.extend(selected_categorical_label_list)\n",
        "\n",
        "    # Convert the balanced lists to NumPy arrays for consistent output format\n",
        "    return (\n",
        "        np.array(balanced_point_clouds),\n",
        "        np.array(balanced_labels),\n",
        "        np.array(balanced_label_list),\n",
        "        np.array(balanced_categorical_label_list)\n",
        "    )\n",
        "\n",
        "# Example usage:\n",
        "balanced_point_clouds, balanced_labels, balanced_label_list, balanced_categorical_label_list = knn_undersample_3d(\n",
        "     point_clouds, kmeans_labels, all_labels, all_cat)\n",
        "\n",
        "# Output the results\n",
        "print(\"Balanced point clouds shape:\", balanced_point_clouds.shape)  # Expected shape: (num_samples, n_points, 3)\n",
        "print(\"Balanced labels distribution:\", np.unique(balanced_labels, return_counts=True))\n",
        "print(\"Balanced label_list_shape:\", balanced_label_list.shape)  # Should match balanced labels in size and alignment\n",
        "print(\"Balanced categorical_label_list_shape:\", balanced_categorical_label_list.shape)  # Should align with balanced samples\n"
      ],
      "metadata": {
        "id": "eRE9pVA6-hk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    point_clouds = balanced_point_clouds\n",
        "    all_cat= balanced_categorical_label_list\n",
        "    all_labels= balanced_label_list"
      ],
      "metadata": {
        "id": "0OSkCPWjOopl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_data_quality(point_clouds, all_labels):\n",
        "\n",
        "    num_points = point_clouds[0].shape[0]\n",
        "    inconsistent_samples = []\n",
        "\n",
        "    #Verify Point Count\n",
        "    for i in range(len(point_clouds)):\n",
        "        if point_clouds[i].shape[0] != num_points:\n",
        "             inconsistent_samples.append(i)\n",
        "             print(f\"Point cloud {i} has {point_clouds[i].shape[0]} points (expected {num_points}).\")\n",
        "\n",
        "    #Check for Missing Values\n",
        "    missing_labels = np.isnan(all_labels)\n",
        "    if np.any(missing_labels):\n",
        "        print(\"There are missing values in the labels.\")\n",
        "\n",
        "    #Visual Inspection of a few point clouds\n",
        "    visualize_m_samples(point_clouds, all_labels)\n",
        "\n",
        "    #Remove Inconsistent Samples\n",
        "    if inconsistent_samples:\n",
        "        print(f\"Removing inconsistent samples: {inconsistent_samples}\")\n",
        "        point_clouds_cleaned = np.delete(point_clouds, inconsistent_samples, axis=0)\n",
        "        all_labels_cleaned = np.delete(all_labels, inconsistent_samples, axis=0)\n",
        "    else:\n",
        "        point_clouds_cleaned = point_clouds\n",
        "        all_labels_cleaned = all_labels\n",
        "\n",
        "    # Return cleaned datasets\n",
        "    return point_clouds_cleaned, all_labels_cleaned\n",
        "\n",
        "def visualize_m_samples(point_clouds, all_labels, num_samples=5):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(num_samples):\n",
        "        ax = plt.subplot(1, num_samples, i + 1, projection='3d')\n",
        "        ax.scatter(point_clouds[i][:, 0], point_clouds[i][:, 1], point_clouds[i][:, 2], c=all_labels[i], alpha=0.5)\n",
        "        ax.set_title(f'Point Cloud {i}')\n",
        "    plt.show()\n",
        "\n",
        "def detect_outliers(point_clouds):\n",
        "\n",
        "    point_clouds = np.array(point_clouds)\n",
        "\n",
        "    # Initialize a mask for outliers\n",
        "    outlier_mask = np.zeros(point_clouds.shape, dtype=bool)\n",
        "\n",
        "    # Iterate over each point cloud\n",
        "    for i in range(point_clouds.shape[0]):\n",
        "        # Calculate the mean and standard deviation for each coordinate\n",
        "        mean = np.mean(point_clouds[i], axis=0)  # Mean for each coordinate (x, y, z)\n",
        "        std_dev = np.std(point_clouds[i], axis=0, ddof=1)  # Std deviation for each coordinate\n",
        "\n",
        "        # Calculate Z-scores for the current point cloud\n",
        "        z_scores = (point_clouds[i] - mean) / std_dev\n",
        "\n",
        "        # Create a mask for points where any coordinate's Z-score is greater than 3\n",
        "        outlier_mask[i] = np.abs(z_scores) > 3\n",
        "\n",
        "    # Remove outliers from point clouds and corresponding labels\n",
        "    cleaned_point_clouds = []\n",
        "    cleaned_labels = []\n",
        "    cleaned_label = [[]]\n",
        "\n",
        "    for i in range(point_clouds.shape[0]):\n",
        "        # Keep only points that are not outliers\n",
        "        cleaned_point_cloud = point_clouds[i][~outlier_mask[i]]\n",
        "        #print(i, ~outlier_mask[i][0][0])\n",
        "        cleaned_label = all_labels[i][~outlier_mask[i][0][0]]  # Directly access labels\n",
        "        cleaned_point_clouds.append(cleaned_point_cloud)\n",
        "        cleaned_labels.append(cleaned_label)\n",
        "\n",
        "    # Convert lists back to NumPy arrays\n",
        "    return np.array(cleaned_point_clouds), np.array(cleaned_labels)\n",
        "\n",
        "\n",
        "\n",
        "point_clouds_cleaned, all_labels_cleaned = check_data_quality(point_clouds=point_clouds, all_labels= all_labels)\n",
        "point_clouds_cleaned, all_labels_cleaned = detect_outliers(point_clouds_cleaned)"
      ],
      "metadata": {
        "id": "HTbEDwI2OiSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cB9FuBYTkr0"
      },
      "source": [
        "##Class distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = {0: 'other', 1: 'wall', 2: 'roof', 3: 'floor'}\n",
        "\n",
        "# Convert point clouds and labels to numpy arrays\n",
        "point_clouds = np.array(point_clouds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Flatten labels for class distribution check\n",
        "flattened_labels = all_labels.flatten()\n",
        "\n",
        "def plot_class_distribution(labels):\n",
        "    counter = Counter(labels)\n",
        "    classes, counts = zip(*counter.items())\n",
        "\n",
        "    # Map class indices to class names\n",
        "    class_labels = [class_names[i] for i in classes]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(class_labels, counts, color='lightblue')\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.title('Class Distribution')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Plot original class distribution\n",
        "plot_class_distribution(flattened_labels)"
      ],
      "metadata": {
        "id": "9-CfsA5FnZFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHtP2Kl_Tkr1"
      },
      "source": [
        "### Creating TensorFlow datasets\n",
        "\n",
        "We create `tf.data.Dataset` objects for the training and validation data.\n",
        "We also augment the training point clouds by applying random jitter to them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = int(len(point_clouds) * (1 - VAL_SPLIT))\n",
        "train_point_clouds = point_clouds[:split_index]\n",
        "train_label_cloud = all_cat[:split_index]\n",
        "total_training_examples = len(train_point_clouds)\n",
        "\n",
        "val_point_clouds = point_clouds[split_index:]\n",
        "val_label_cloud = all_cat[split_index:]\n",
        "\n",
        "print(\"Num train point clouds:\", len(train_point_clouds))\n",
        "print(\"Num train point cloud labels:\", len(train_label_cloud))\n",
        "print(\"Num val point clouds:\", len(val_point_clouds))\n",
        "print(\"Num val point cloud labels:\", len(val_label_cloud))"
      ],
      "metadata": {
        "id": "f2L4sE79_-X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7zLrGabTkr1"
      },
      "outputs": [],
      "source": [
        "def load_data(point_cloud_batch, label_cloud_batch):\n",
        "    point_cloud_batch.set_shape([NUM_SAMPLE_POINTS, 3])\n",
        "    label_cloud_batch.set_shape([NUM_SAMPLE_POINTS, len(LABELS) + 1])\n",
        "    return point_cloud_batch, label_cloud_batch\n",
        "\n",
        "\n",
        "def augment(point_cloud_batch, label_cloud_batch):\n",
        "    noise = tf.random.uniform(\n",
        "        tf.shape(label_cloud_batch), -0.001, 0.001, dtype=tf.float64\n",
        "    )\n",
        "    point_cloud_batch += noise[:, :, :3]\n",
        "    return point_cloud_batch, label_cloud_batch\n",
        "\n",
        "\n",
        "def generate_dataset(point_clouds, label_clouds, is_training=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((point_clouds, label_clouds))\n",
        "    dataset = dataset.shuffle(BATCH_SIZE * 100) if is_training else dataset\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size=BATCH_SIZE)\n",
        "    dataset = (\n",
        "        dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        if is_training\n",
        "        else dataset\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "train_dataset = generate_dataset(train_point_clouds, train_label_cloud)\n",
        "val_dataset = generate_dataset(val_point_clouds, val_label_cloud, is_training=False)\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Validation Dataset:\", val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnz8PaYxTkr1"
      },
      "source": [
        "# PointNet model training prep\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pointnet model"
      ],
      "metadata": {
        "id": "sypRPKwuTiLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po7cNOvVTkr1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def conv_block(x, filters, name):\n",
        "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\", name=f\"{name}_conv\")(x)\n",
        "    x = layers.BatchNormalization(name=f\"{name}_batch_norm\")(x)\n",
        "    return layers.Activation(\"relu\", name=f\"{name}_relu\")(x)\n",
        "\n",
        "\n",
        "def mlp_block(x, filters, name):\n",
        "    x = layers.Dense(filters, name=f\"{name}_dense\")(x)\n",
        "    x = layers.BatchNormalization(name=f\"{name}_batch_norm\")(x)\n",
        "    return layers.Activation(\"relu\", name=f\"{name}_relu\")(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stVBhfO4Tkr2"
      },
      "outputs": [],
      "source": [
        "\n",
        "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
        "    \"\"\"Reference: https://keras.io/examples/vision/pointnet/#build-a-model\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, l2reg=0.001):\n",
        "        self.num_features = num_features\n",
        "        self.l2reg = l2reg\n",
        "        self.identity = keras.ops.eye(num_features)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = keras.ops.reshape(x, (-1, self.num_features, self.num_features))\n",
        "        xxt = keras.ops.tensordot(x, x, axes=(2, 2))\n",
        "        xxt = keras.ops.reshape(xxt, (-1, self.num_features, self.num_features))\n",
        "        return keras.ops.sum(self.l2reg * keras.ops.square(xxt - self.identity))\n",
        "\n",
        "   # def get_config(self):\n",
        "   #     config = super().get_config()\n",
        "   #     config.update({\"num_features\": self.num_features, \"l2reg_strength\": self.l2reg})\n",
        "   #     return config\n",
        "\n",
        "    def get_config(self):\n",
        "        # Return the regularizer's configuration as a dictionary\n",
        "        return {\n",
        "            \"num_features\": self.num_features,\n",
        "            \"l2reg_strength\": self.l2reg\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGEdfkrNTkr2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def transformation_net(inputs, num_features, name):\n",
        "    \"\"\"\n",
        "    Reference: https://keras.io/examples/vision/pointnet/#build-a-model.\n",
        "\n",
        "    The `filters` values come from the original paper:\n",
        "    https://arxiv.org/abs/1612.00593.\n",
        "    \"\"\"\n",
        "    x = conv_block(inputs, filters=64, name=f\"{name}_1\")\n",
        "    x = conv_block(x, filters=128, name=f\"{name}_2\")\n",
        "    x = conv_block(x, filters=1024, name=f\"{name}_3\")\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = mlp_block(x, filters=512, name=f\"{name}_1_1\")\n",
        "    x = mlp_block(x, filters=256, name=f\"{name}_2_1\")\n",
        "    return layers.Dense(\n",
        "        num_features * num_features,\n",
        "        kernel_initializer=\"zeros\",\n",
        "        bias_initializer=keras.initializers.Constant(np.eye(num_features).flatten()),\n",
        "        activity_regularizer=OrthogonalRegularizer(num_features),\n",
        "        name=f\"{name}_final\",\n",
        "    )(x)\n",
        "\n",
        "\n",
        "def transformation_block(inputs, num_features, name):\n",
        "    transformed_features = transformation_net(inputs, num_features, name=name)\n",
        "    transformed_features = layers.Reshape((num_features, num_features))(\n",
        "        transformed_features\n",
        "    )\n",
        "    return layers.Dot(axes=(2, 1), name=f\"{name}_mm\")([inputs, transformed_features])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PtHuS4tTkr2"
      },
      "source": [
        "Finally, we piece the above blocks together and implement the segmentation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVu1lXy3Tkr2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_shape_segmentation_model(num_points, num_classes):\n",
        "    input_points = keras.Input(shape=(None, 3))\n",
        "\n",
        "    # PointNet Classification Network.\n",
        "    transformed_inputs = transformation_block(\n",
        "        input_points, num_features=3, name=\"input_transformation_block\"\n",
        "    )\n",
        "    features_64 = conv_block(transformed_inputs, filters=64, name=\"features_64\")\n",
        "    features_128_1 = conv_block(features_64, filters=128, name=\"features_128_1\")\n",
        "    features_128_2 = conv_block(features_128_1, filters=128, name=\"features_128_2\")\n",
        "    transformed_features = transformation_block(\n",
        "        features_128_2, num_features=128, name=\"transformed_features\"\n",
        "    )\n",
        "    features_512 = conv_block(transformed_features, filters=512, name=\"features_512\")\n",
        "    features_2048 = conv_block(features_512, filters=2048, name=\"pre_maxpool_block\")\n",
        "    global_features = layers.MaxPool1D(pool_size=num_points, name=\"global_features\")(\n",
        "        features_2048\n",
        "    )\n",
        "    global_features = keras.ops.tile(global_features, [1, num_points, 1])\n",
        "\n",
        "    # Segmentation head.\n",
        "    segmentation_input = layers.Concatenate(name=\"segmentation_input\")(\n",
        "        [\n",
        "            features_64,\n",
        "            features_128_1,\n",
        "            features_128_2,\n",
        "            transformed_features,\n",
        "            features_512,\n",
        "            global_features,\n",
        "        ]\n",
        "    )\n",
        "    segmentation_features = conv_block(\n",
        "        segmentation_input, filters=128, name=\"segmentation_features\"\n",
        "    )\n",
        "    outputs = layers.Conv1D(\n",
        "        num_classes, kernel_size=1, activation=\"softmax\", name=\"segmentation_head\"\n",
        "    )(segmentation_features)\n",
        "    return keras.Model(input_points, outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM7oVqkmTkr2"
      },
      "source": [
        "## Instantiate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viupEHO0Tkr2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(train_dataset))\n",
        "\n",
        "num_points = x.shape[1]\n",
        "num_classes = y.shape[-1]\n",
        "\n",
        "segmentation_model = get_shape_segmentation_model(num_points, num_classes)\n",
        "segmentation_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4RKAnRGTkr3"
      },
      "source": [
        "## Training\n",
        "\n",
        "For the training the authors recommend using a learning rate schedule that decays the\n",
        "initial learning rate by half every 20 epochs. In this example, we use 60 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKQPK9h9Tkr3"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = total_training_examples // BATCH_SIZE\n",
        "total_training_steps = steps_per_epoch * EPOCHS\n",
        "print(f\"Steps per epoch: {steps_per_epoch}.\")\n",
        "print(f\"Total training steps: {total_training_steps}.\")\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.003,\n",
        "    decay_steps=steps_per_epoch * 5,\n",
        "    decay_rate=0.5,\n",
        "    staircase=True,\n",
        ")\n",
        "\n",
        "steps = range(total_training_steps)\n",
        "lrs = [lr_schedule(step) for step in steps]\n",
        "\n",
        "plt.plot(lrs)\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcoWfKwBTkr3"
      },
      "source": [
        "Finally, we implement a utility for running our experiments and launch model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORtVaT9uTkr3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def run_experiment(epochs):\n",
        "    segmentation_model = get_shape_segmentation_model(num_points, num_classes)\n",
        "    segmentation_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "        loss=keras.losses.CategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    # Define file paths for saving the model and weights\n",
        "    checkpoint_filepath = \"checkpoint.weights.h5\"\n",
        "    model_save_path = \"path to save the model/segmentation_model.h5\"  # Path to save the full model\n",
        "    weights_save_path = \"path to save the model/segmentation_model.weights.h5\"  # Path to save just the weights\n",
        "\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = segmentation_model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=epochs,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    segmentation_model.load_weights(checkpoint_filepath)\n",
        "\n",
        "     # Save the entire model (architecture + weights + optimizer state)\n",
        "    segmentation_model.save(model_save_path)\n",
        "\n",
        "    # Optionally, you can also save just the weights\n",
        "    segmentation_model.save_weights(weights_save_path)\n",
        "    return segmentation_model, history\n",
        "\n",
        "\n",
        "segmentation_model, history = run_experiment(epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results of Training"
      ],
      "metadata": {
        "id": "T-1gMGR_Tu6P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9Ds8zVKTkr3"
      },
      "source": [
        "## Visualize the training landscape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_p = 'path to save the plots/Plots/'\n",
        "\n",
        "\n",
        "def plot_result(item, save_path=None):\n",
        "    plt.plot(history.history[item], label=item)\n",
        "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    # Save the plot if a save_path is provided\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: Plot and save loss and accuracy plots\n",
        "plot_result(\"loss\", save_path= save_p + \"loss_plot.png\")\n",
        "plot_result(\"accuracy\", save_path= save_p + \"accuracy_plot.png\")"
      ],
      "metadata": {
        "id": "6JzHiNU3G1LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FfjynYBTkr7"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_batch = next(iter(val_dataset))\n",
        "val_predictions = segmentation_model.predict(validation_batch[0])\n",
        "print(f\"Validation prediction shape: {val_predictions.shape}\")\n",
        "\n",
        "def visualize_data(point_cloud, labels, save_path = None):\n",
        "    df = pd.DataFrame(\n",
        "        data={\n",
        "            \"x\": point_cloud[:, 0],\n",
        "            \"y\": point_cloud[:, 1],\n",
        "            \"z\": point_cloud[:, 2],\n",
        "            \"label\": labels,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    ax = plt.axes(projection=\"3d\")\n",
        "\n",
        "    colors = ['black','green', 'purple', 'blue']\n",
        "\n",
        "    #print (len(df))\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        x, y, z = df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['z']\n",
        "        col = int(df.iloc[i]['label'])\n",
        "        ax.scatter(x, y, z, alpha = 0.5, c=colors[col]\n",
        "            )\n",
        "\n",
        "    # Save the plot if a save_path is provided\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_single_point_cloud(point_clouds, label_clouds, idx, save_path=None):\n",
        "    label_map = LABELS + [\"none\"]\n",
        "    print(label_map)\n",
        "    point_cloud = point_clouds[idx]\n",
        "    label_cloud = label_clouds[idx]\n",
        "    original_labels = np.argmax(label_cloud, axis=1)\n",
        "    print(original_labels)\n",
        "    visualize_data(point_cloud, original_labels, save_path)\n",
        "\n",
        "\n",
        "idx = np.random.choice(len(validation_batch[0]))\n",
        "print(f\"Index selected: {idx}\")\n",
        "\n",
        "# Plotting with ground-truth.\n",
        "visualize_single_point_cloud(validation_batch[0], validation_batch[1], idx, save_path = save_p + 'ground_trouth.png')\n",
        "\n",
        "# Plotting with predicted labels.\n",
        "visualize_single_point_cloud(validation_batch[0], val_predictions, idx, save_path = save_p + 'predictions.png')"
      ],
      "metadata": {
        "id": "dS6kzEzABIlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_misclassified_points(point_cloud, true_labels, predicted_labels, save_path=None):\n",
        "    df = pd.DataFrame(\n",
        "        data={\n",
        "            \"x\": point_cloud[:, 0],\n",
        "            \"y\": point_cloud[:, 1],\n",
        "            \"z\": point_cloud[:, 2],\n",
        "            \"true_label\": true_labels,\n",
        "            \"pred_label\": predicted_labels,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    ax = plt.axes(projection=\"3d\")\n",
        "\n",
        "    # Define the colors for the true labels and red for misclassified points\n",
        "    colors = ['black', 'green', 'purple', 'blue']  # Colors for true labels\n",
        "    red_color = 'red'  # Color for incorrect predictions\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        x, y, z = df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['z']\n",
        "        true_label = int(df.iloc[i]['true_label'])\n",
        "        pred_label = int(df.iloc[i]['pred_label'])\n",
        "\n",
        "        # Check if prediction is correct; if not, color the point red\n",
        "        if true_label != pred_label:\n",
        "            ax.scatter(x, y, z, alpha=0.8, c=red_color, label=\"Misclassified\" if i == 0 else \"\")\n",
        "        else:\n",
        "            ax.scatter(x, y, z, alpha=0.5, c=colors[true_label])\n",
        "\n",
        "    # Save the plot if a save_path is provided\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_single_point_cloud_with_misclass(point_clouds, label_clouds, predicted_labels, idx, save_path=None):\n",
        "    label_map = LABELS + [\"none\"]\n",
        "    print(label_map)\n",
        "\n",
        "    point_cloud = point_clouds[idx]\n",
        "    true_label_cloud = label_clouds[idx]\n",
        "    predicted_label_cloud = predicted_labels[idx]\n",
        "\n",
        "    # Convert one-hot encoded ground-truth labels to integer labels\n",
        "    true_labels = np.argmax(true_label_cloud, axis=1)\n",
        "    predicted_labels = np.argmax(predicted_label_cloud, axis=1)\n",
        "\n",
        "    # Visualize the misclassified points\n",
        "    visualize_misclassified_points(point_cloud, true_labels, predicted_labels, save_path)\n",
        "\n",
        "\n",
        "# Index selection\n",
        "idx = np.random.choice(len(validation_batch[0]))\n",
        "print(f\"Index selected: {idx}\")\n",
        "\n",
        "# Plotting with misclassified points highlighted\n",
        "visualize_single_point_cloud_with_misclass(validation_batch[0], validation_batch[1], val_predictions, idx, save_path= save_p + 'missclasified_point.png')"
      ],
      "metadata": {
        "id": "Xmbtu4IVB6zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define labels for architectural elements\n",
        "class_labels = [\"None\", \"Wall\", \"Roof\", \"Floor\"]\n",
        "\n",
        "def visualize_misclassified_points_interactive(point_cloud, true_labels, predicted_labels, save_path=None):\n",
        "    df = pd.DataFrame(\n",
        "        data={\n",
        "            \"x\": point_cloud[:, 0],\n",
        "            \"y\": point_cloud[:, 1],\n",
        "            \"z\": point_cloud[:, 2],\n",
        "            \"true_label\": true_labels,\n",
        "            \"pred_label\": predicted_labels,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Define colors for each class (Wall, Roof, Floor) and red for misclassified points\n",
        "    correct_colors = ['black', 'olivedrab', 'mediumpurple', 'mediumblue']  # Colors corresponding to Wall, Roof, Floor\n",
        "    misclassified_color = 'orangered'  # Color for misclassified points\n",
        "\n",
        "    # Separate the misclassified and correctly classified points\n",
        "    misclassified_points = df[df[\"true_label\"] != df[\"pred_label\"]]\n",
        "    correct_points = df[df[\"true_label\"] == df[\"pred_label\"]]\n",
        "\n",
        "    # Debug: Check counts of each class\n",
        "    #print(\"Counts of correctly classified points per class:\")\n",
        "    #for label in (np.unique(true_labels)-1):\n",
        "        #class_count = len(correct_points[correct_points[\"true_label\"] == label])\n",
        "        #print(f\"{class_labels[label]}: {class_count} points\")\n",
        "\n",
        "    # Create the Plotly figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Plot correctly classified points for each class (Wall, Roof, Floor)\n",
        "    for label, color in enumerate(correct_colors):\n",
        "        class_points = correct_points[correct_points[\"true_label\"] == label]\n",
        "\n",
        "        # Only plot if there are points for the current class\n",
        "        if not class_points.empty:\n",
        "            fig.add_trace(go.Scatter3d(\n",
        "                x=class_points[\"x\"],\n",
        "                y=class_points[\"y\"],\n",
        "                z=class_points[\"z\"],\n",
        "                mode='markers',\n",
        "                marker=dict(size=4, color=color, opacity=0.7),\n",
        "                name=f'{class_labels[label]} (Correct)'\n",
        "            ))\n",
        "\n",
        "    # Plot misclassified points in red\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=misclassified_points[\"x\"],\n",
        "        y=misclassified_points[\"y\"],\n",
        "        z=misclassified_points[\"z\"],\n",
        "        mode='markers',\n",
        "        marker=dict(size=5, color=misclassified_color, opacity=0.9),\n",
        "        name=\"Misclassified\"\n",
        "    ))\n",
        "\n",
        "    # Update layout to remove background axes, grid, and ticks\n",
        "    fig.update_layout(\n",
        "        title=\"3D Point Cloud with Misclassified Points Highlighted\",\n",
        "        scene=dict(\n",
        "            xaxis=dict(visible=False),\n",
        "            yaxis=dict(visible=False),\n",
        "            zaxis=dict(visible=False),\n",
        "            bgcolor=\"rgba(0,0,0,0)\"  # Transparent background\n",
        "        ),\n",
        "        showlegend=True,  # Keeps the legend\n",
        "    )\n",
        "\n",
        "    # Save the plot as an HTML file if save_path is provided\n",
        "    if save_path:\n",
        "        fig.write_html(save_path)\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "def visualize_single_point_cloud_with_misclass_interactive(point_clouds, label_clouds, predicted_labels, idx, save_path=None):\n",
        "    point_cloud = point_clouds[idx]\n",
        "    true_label_cloud = label_clouds[idx]\n",
        "    predicted_label_cloud = predicted_labels[idx]\n",
        "\n",
        "    # Convert one-hot encoded ground-truth labels to integer labels\n",
        "    true_labels = np.argmax(true_label_cloud, axis=1)\n",
        "    predicted_labels = np.argmax(predicted_label_cloud, axis=1)\n",
        "\n",
        "    # Visualize the misclassified points interactively\n",
        "    visualize_misclassified_points_interactive(point_cloud, true_labels, predicted_labels, save_path)\n",
        "\n",
        "# Randomly select an index and visualize\n",
        "idx = np.random.choice(len(validation_batch[0]))\n",
        "print(f\"Index selected: {idx}\")\n",
        "\n",
        "\n",
        "visualize_single_point_cloud_with_misclass_interactive(\n",
        "    validation_batch[0], validation_batch[1], val_predictions, idx\n",
        ")"
      ],
      "metadata": {
        "id": "r1xnhYgvi5t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implement pre-trained model"
      ],
      "metadata": {
        "id": "z8YCh8pW1Eo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the model"
      ],
      "metadata": {
        "id": "IB6HojWFsSpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the custom regularizer with the correct decorator\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class OrthogonalRegularizer(Regularizer):\n",
        "    def __init__(self, num_features, l2reg_strength=0.001):\n",
        "        self.num_features = num_features\n",
        "        self.l2reg_strength = l2reg_strength\n",
        "\n",
        "    def __call__(self, x):\n",
        "        regularization = self.l2reg_strength * tf.reduce_sum(tf.square(x))\n",
        "        return regularization\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"num_features\": self.num_features, \"l2reg_strength\": self.l2reg_strength}\n",
        "\n",
        "# Custom Tile Layer with 'repeats' as a configurable parameter\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class Tile(Layer):\n",
        "    def __init__(self, repeats, **kwargs):\n",
        "        super(Tile, self).__init__(**kwargs)\n",
        "        self.repeats = repeats\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.tile(inputs, multiples=self.repeats)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Tile, self).get_config()\n",
        "        config.update({\n",
        "            'repeats': self.repeats\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Now load the model with both custom objects\n",
        "model = load_model(\n",
        "    '/content/gdrive/MyDrive/Colab_Notebooks/Pointcloud_to _BIM/segmentation_model.h5',\n",
        "    custom_objects={\n",
        "        'OrthogonalRegularizer': OrthogonalRegularizer,\n",
        "        'Tile': Tile\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "9Vd9ZYhg2dfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict values"
      ],
      "metadata": {
        "id": "RDpcMwqhsVh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.predict(np.array( point_clouds))\n",
        "\n",
        "# Convert class probabilities to class labels\n",
        "predicted_labels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "# predicted_labels will have shape (num_samples, num_points) with class predictions for each point\n",
        "print(predicted_labels)"
      ],
      "metadata": {
        "id": "hxq7QCB453OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculate mIoU"
      ],
      "metadata": {
        "id": "m3iQdGTjsbLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intersection over Union (IoU) is a metric used to evaluate the accuracy of object detection and segmentation tasks, particularly in scenarios where we need to compare predicted labels against ground truth labels. For each class, IoU is calculated as the ratio of the intersection of the predicted and ground truth areas to the union of these areas. Specifically, it quantifies the overlap between the predicted and actual regions corresponding to a particular class, providing a measure of how well the predicted segmentation aligns with the true segmentation. A higher IoU indicates better model performance for that class.\n",
        "\n",
        "Mean Intersection over Union (mIoU) is the average of IoUs calculated across all classes. It serves as a comprehensive performance metric, capturing the overall ability of a model to segment various classes effectively. By averaging the IoUs, mIoU accounts for the performance on each class individually, allowing us to assess how well the model generalizes across all categories, even if some classes may be harder to predict than others. In our context, we use IoU and mIoU to evaluate the effectiveness of our point cloud segmentation model by comparing the predicted labels to the ground truth labels, helping us identify strengths and weaknesses in the model's segmentation capabilities."
      ],
      "metadata": {
        "id": "uz1tMn7ErhHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou_per_class(pred_labels, gt_labels, num_classes):\n",
        "    ious = []\n",
        "    for c in range(num_classes):\n",
        "        # Find points in the ground truth and predictions for this class\n",
        "        intersection = np.sum((pred_labels == c) & (gt_labels == c))  # Intersection\n",
        "        union = np.sum((pred_labels == c) | (gt_labels == c))         # Union\n",
        "\n",
        "        if union == 0:\n",
        "            iou = float('nan')  # If no points belong to this class in ground truth\n",
        "        else:\n",
        "            iou = intersection / union\n",
        "\n",
        "        ious.append(iou)\n",
        "\n",
        "    return ious\n",
        "\n",
        "def calculate_miou(pred_labels, gt_labels, num_classes):\n",
        "    \"\"\"Calculate the mean IoU.\"\"\"\n",
        "    ious = calculate_iou_per_class(pred_labels, gt_labels, num_classes)\n",
        "    # Filter out NaNs (which represent classes not present in ground truth)\n",
        "    valid_ious = [iou for iou in ious if not np.isnan(iou)]\n",
        "\n",
        "    # Compute mIoU\n",
        "    miou = np.mean(valid_ious)\n",
        "    return miou, ious\n",
        "\n",
        "# Example usage with a point cloud dataset\n",
        "num_classes = 4  # Adjust to the number of classes in your dataset\n",
        "\n",
        "# Calculate mIoU\n",
        "miou, ious_per_class = calculate_miou(predicted_labels, all_labels, num_classes)\n",
        "\n",
        "print(f\"mIoU: {miou}\")\n",
        "print(f\"IoU per class: {ious_per_class}\")"
      ],
      "metadata": {
        "id": "wbPcvUfV1EaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The calculated mean Intersection over Union (mIoU) of approximately **0.802** indicates that our point cloud segmentation model performs well overall, effectively capturing the spatial characteristics of the different classes in the dataset. This score suggests that the model achieves a good balance between precision and recall across various classes.\n",
        "\n",
        "However, examining the IoU scores per class reveals some important insights.\n",
        "\n",
        "The first class (rood) demonstrates an IoU of approximately **0.804**, reflecting a strong performance in segmenting this class and suggesting that the model effectively identifies and delineates its boundaries. The second class (wall), with an IoU of around **0.880**, is the best-performing class, indicating that the model excels in recognizing and segmenting this particular category, potentially due to clearer features or more substantial training data associated with it.\n",
        "\n",
        "In contrast, the third class (floor) scores an IoU of about **0.720**, which, while still reasonable, suggests some challenges in accurately predicting this class. This lower score could indicate that the model struggles with the variability of the fourth class or that it may require more targeted training data or further tuning to improve its segmentation accuracy. Overall, while the model shows promising results, the variation in IoU scores highlights areas for potential improvement, especially for the fourth class and the need to ensure all classes are represented in both training and evaluation datasets."
      ],
      "metadata": {
        "id": "kyworGyHr4nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize mIoU per class"
      ],
      "metadata": {
        "id": "-MxBQXFa8WRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['Other', 'Roof', 'Wall', 'floor']\n",
        "mIoU_values = ious_per_class\n",
        "\n",
        "# Create a DataFrame for better visualization with Seaborn\n",
        "mIoU_df = pd.DataFrame({'Class': classes, 'mIoU': mIoU_values})\n",
        "\n",
        "# Set the aesthetics for the plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "barplot = sns.barplot(x='Class', y='mIoU', data=mIoU_df, palette='viridis')\n",
        "\n",
        "# Add data labels on top of the bars\n",
        "for p in barplot.patches:\n",
        "    barplot.annotate(format(p.get_height(), '.2f'),\n",
        "                     (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                     ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Mean IoU per Class', fontsize=16)\n",
        "plt.xlabel('Classes', fontsize=14)\n",
        "plt.ylabel('Mean IoU', fontsize=14)\n",
        "plt.ylim(0, 1)  # Setting the y-axis limits from 0 to 1\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WUlo8L6874KQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "jHtP2Kl_Tkr1",
        "sypRPKwuTiLq",
        "XM7oVqkmTkr2",
        "z8YCh8pW1Eo5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}